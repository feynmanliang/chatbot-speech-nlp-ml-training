### AI, Speech Recognition, ML, NLP
#### Breaking down a dialogue system

Feynman Liang, Gigster, 15 Nov 2017



### Running example

![](images/dialogue_system.png)


### Agenda

 * Simple dialogue management
  * Rule-based AI, DFAs
 * Speech recognition
  * Bayes Rule, HMMs
  * Phoneme classification, DNNs
  * Language modelling, NFAs
  * Case study: Amazon Alexa
 * Semantic decoding
  * NLP
 * Dialogue state management
  * RNNs
  * Reinforcement learning



## Simple dialogue management


### Deterministic finite automata (DFA)

 * $Q$, finite set of states
 * $\Sigma$, alphabet (transitions)

![](images/dfa-odd-zeros.png)


### Rule-based systems: vending machine

![](images/vending_machine.png)


## Rule-based systems: TCP

![](images/dfa-tcp.png)


### Rule-based systems: Pacman AI

![](images/dfa-pacman.png)


### Eliza (Weizenbaum 1996)

 * Input sentences by decomposition rules triggered by key words appearing.
 * Responses generated by reassembly rules associated with selected decomposition rules.

[DEMO: Eliza](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm)



## Speech Recognition


![](images/spectrogram.png)


### Bayes Rule

$$P(Y | X) = \frac{P(X | Y) P(Y)}{P(X)} \propto P(X | Y) P(Y)$$


### Bayes Rule in speech recognition

$$P(words | speech) = P(speech | words) P(words)$$

 * $P(speech | words)$, acoustic model
 * $P(words)$, language model



## Aside: classification


### Classiciation

Given data $x\_i \in \mathbb{R}^d$ and labels $y\_i \in \\{1,2,\ldots,K\\}$,
"learn" a "good" **classifier** $f : \mathbb{R}^d \to \\{1,2,\ldots,K\\}$


### Example: "Given speech, predict the words"

$$P(words | speech)$$


### Example: classifying handwritten numbers

![](images/typical-supervised.png)


### Typical supervised learning pipeline

![](images/supervised-diagram.png)


### What is "good"?

Typically measured through a **loss function** $L(f(x\_i), y\_i)$


### Example: mean absolute error

$L(f(x\_i), y\_i) = \lvert f(x\_i) - y\_i \rvert$
![](images/residuals.png)


### How to "learn"?

Optimize: if $\theta$ are the parameters of the model, solve

$\text{argmin}\_\theta \sum\_i L(f\_\theta(x\_i), y\_i)$


### Gradient descent optimization

![](images/gradient_descent.png)



## Aside: Deep Neural Networks


## State of the art: DNN acoustic models

![](images/dnn-am-frontcover.png)


### Neurons

![](images/neuron.png)


### Neural networks

![](images/ffw.png)


## In matrix notation

$$y^{(l)} = \sigma(W^{(l)} y^{(l-1)})$$

$$h^{(0)} = x$$

Optimize $\theta = \\{W^{(l)}\\}\_{l=1}^L$.



## Back to speech recognition


### Speech signal to features $x \in \mathbb{R}^d$

![](images/feature-transform.png)


### DNN acoustic model for $P(words | speech)$

![](images/dnn-classification.png)


### Language modelling

Markovian structure:

$P(w\_{1:L}) = \prod\_{i=1}^L P(w\_i | w\_{1:i-1})$


### $n$-gram models

$P(w\_i | w\_{1:i-1}) = P(w\_i | w\_{i-1})$

$P(w\_i | w\_{i-1}) = \frac{P(w\_i, w\_{i-1})}{\sum\_{\tilde{w}} P(w\_i, \tilde{w})}$


[DEMO: NGram analyzer](http://guidetodatamining.com/ngramAnalyzer/)

[DEMO: NGram generator](https://blog.algorithmia.com/playing-with-n-grams/)


### Recurrent neural networks (RNNs): memory cell

![](images/memory-cell.png)


### RNNs

![](images/stacked-rnn.png)


## RNNs

![](images/charseq.jpeg)


Agent: container of intents, entities, responses

User input -> Text
 * Speech recognition

Request -> Intent
 * Called NLU (natural language understanding)
 * NLP: different phrases, same meaning
 * NER: entity recognition
 * CUED-DS: intents = actions, entities = slots

Intent -> Fulfillment -> Response
 * Information Retrieval
 * ML

Online interaction
 * Conversation state / context

---

## Finite state machines

---

## Speech recognition

---

## Amazon Alexa

---

## Dialogue Systems

NLU: Request -> Intent


